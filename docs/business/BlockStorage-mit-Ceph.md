

## üß† **Ziel:** Ceph als Ersatz f√ºr Pi-basierte NFS/MinIO in deiner sp√§teren OpenStack-Produktionsumgebung

üì¶ *Ceph soll dienen f√ºr: RBD (Block Storage), CephFS (/shared), RGW (S3)*

---

## üí° **Minimale Hardware-Anforderungen (f√ºr Test + kleine Produktion)**

| Rolle        | CPU    | RAM  | Disk Setup              | Kommentar                          |
| ------------ | ------ | ---- | ----------------------- | ---------------------------------- |
| **Ceph MON** | 2 vCPU | 4 GB | SSD                     | Metadata/Quorum, kein Datenvolumen |
| **Ceph MGR** | 2 vCPU | 2 GB | ‚Äì                       | Verwaltungs-Dienst                 |
| **Ceph OSD** | 4 vCPU | 8 GB | 1√ó SSD oder HDD per OSD | Haupt-Storage pro Device           |
| **Ceph MDS** | 2 vCPU | 4 GB | ‚Äì                       | Nur n√∂tig f√ºr CephFS               |
| **Ceph RGW** | 2 vCPU | 2 GB | optional                | S3-kompatibles Objekt-Storage      |

---

### üî∏ **Empfohlene Node-Gr√∂√üe pro Ceph-Node (Minimum):**

| Ressource    | Empfehlung pro Node                   |
| ------------ | ------------------------------------- |
| **CPU**      | ‚â• 4 vCPU                              |
| **RAM**      | ‚â• 16 GB                               |
| **Storage**  | 1 SSD f√ºr System, 1+ HDD/SSD f√ºr OSDs |
| **Netzwerk** | 1 Gbit/s (10 Gbit/s empfohlen)        |

> Du brauchst mindestens **3 Nodes** f√ºr ein stabiles Ceph-Cluster mit MON-Quorum!

---

## üìê Beispiel: Minimal-Ceph-Cluster f√ºr dich (3 Nodes)

| Node    | Rolle              | Ger√§te                     |
| ------- | ------------------ | -------------------------- |
| `ceph1` | MON, MGR, OSD      | 1x System-SSD + 1x 2TB HDD |
| `ceph2` | MON, OSD           | 1x System-SSD + 1x 2TB HDD |
| `ceph3` | MON, MDS, OSD, RGW | 1x System-SSD + 1x 2TB HDD |

---

## üìä **Ressourcenverbrauch pro Dienst (realistisch)**

| Dienst   | RAM idle | RAM unter Last | CPU idle | CPU Last |
| -------- | -------- | -------------- | -------- | -------- |
| MON      | \~150 MB | \~300 MB       | <5%      | \~15%    |
| MGR      | \~200 MB | \~400 MB       | <5%      | \~20%    |
| OSD (1x) | \~800 MB | 2‚Äì4 GB         | 10‚Äì40%   | bis 80%  |
| MDS      | \~500 MB | 1‚Äì2 GB         | 5‚Äì20%    | bis 50%  |
| RGW      | \~400 MB | 1 GB           | 5‚Äì15%    | bis 30%  |

---

## ‚ö†Ô∏è Wenn du *jetzt* Ceph testen willst, aber nur 2 Nodes hast:

> Du **kannst tempor√§r** mit einem **reduzierten 1-Node-Ceph-Cluster** arbeiten, **ABER**:

* ‚ùå Kein Redundanz
* ‚ö†Ô∏è Nicht fehlertolerant
* ‚úÖ Gut f√ºr Funktionstests mit CephFS/RBD/RGW

Beispiel auf `node1`:

```bash
# mit cephadm:
cephadm bootstrap --mon-ip 192.168.100.10
ceph orch apply osd --all-available-devices
ceph fs volume create sharedfs
```

---

## üßæ **Fazit: Was brauchst du f√ºr produktiven Ceph-Storage?**

| Ziel                     | Hardware-Anforderung                      |
| ------------------------ | ----------------------------------------- |
| **Testbetrieb (1 Node)** | 4 vCPU, 16GB RAM, 2 SSD/HDD               |
| **Produktiv (3 Nodes)**  | je Node: 4‚Äì8 vCPU, 16‚Äì32GB RAM, 2‚Äì4 Disks |
| **High-Performance**     | 10 Gbit Netz, SSD/PCIe/NVMe f√ºr OSDs      |

---

## ‚úÖ Vorschlag

Wenn du Ceph *ernsthaft produktiv* betreiben willst:

* Warte bis du **mind. 3 Nodes mit >16 GB RAM + SSD/HDD-Kombi** hast
* Nutze **`cephadm` oder `ceph-ansible`** f√ºr automatisiertes Setup
* Beginne mit **RBD + CephFS**, RGW (S3) sp√§ter

---
